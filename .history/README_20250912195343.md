[![Awesome Logo](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
![Visitors](https://komarev.com/ghpvc/?username=worldbench&repo=awesome-vla4ad&label=Hello,%20Visitor%20&color=yellow&style=social)
[![PR's Welcome](https://img.shields.io/badge/PRs-welcome-red.svg?style=flat)](https://github.com/worldbench/awesome-vla4ad/pulls)

# :sunglasses: Awesome VLA for Autonomous Driving

This survey reviews **vision-action (VA)** models and **vision-language-action (VLA)** models for autonomous driving. We ...

For more details, kindly refer to our [paper](https://worldbench.github.io/vla4ad.pdf) and [project page](https://worldbench.github.io/vla4ad). :rocket:


### :books: Citation 

If you find this work helpful for your research, please kindly consider citing our paper:
```bib
@article{survey_vla4ad,
    title   = {Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future},
    author  = {Tianshuai Hu and Xiaolu Liu and Song Wang and Yiyao Zhu and Guoyang Zhao and Jun Cen and Lingdong Kong and Linfeng Li and Xiangtai Li and Shaojie Shen and Jianke Zhu and Dacheng Tao and Junwei Liang},
    journal = {arXiv preprint arXiv:25xx.xxxxx},
    year    = {2025},
}
```


### Table of Contents
- [**1. Vision-Action-Models**](#1-vision-action-models)
  - [End-to-End VA Models]()
  - [World Models]()
- [**2. Vision-Language-Action-Models**](#2-vision-language-action-models)
  - [End-to-End VLA Models]()
  - [Dual Systems]()
- [**3. Datasets \& Benchmarks**]()
- [**4. Applications**](#4-applications)
- [**5. Other Resources**](#5-other-resources)



## 1. Vision-Action-Models

### :one: End-to-End VA Models

| Model | Paper | Venue | Website | GitHub | 
|:-:|:-|:-:|:-:|:-:|
||
| `GenAD` | [![arXiv](https://img.shields.io/badge/arXiv-2403.09630-b31b1b?style=flat-square&logo=arxiv)](https://arxiv.org/abs/2403.09630)<br>GenAD: Generalized Predictive Model for Autonomous Driving | CVPR 2024 | - | [![GitHub](https://img.shields.io/github/stars/OpenDriveLab/DriveAGI)](https://github.com/OpenDriveLab/DriveAGI) |


### :two: World Models



## 2. Vision-Language-Action-Models



## 3. Datasets & Benchmarks



## 4. Applications



## 5. Other Resources




